{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8d77d4",
   "metadata": {},
   "source": [
    "# Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f5a17",
   "metadata": {},
   "source": [
    "# The Why\n",
    "\n",
    "    - In the early langchain days each components in langchain like models, prompts, retrivers, parsers etc.. all had different interface.\n",
    "    - This was because when chatGpt and other LLMs provided APIs they all had different schema for interacting with it, so the langchain was focused on making common interface between different providers.\n",
    "    - Due to which each component they made had different schema within themselves.\n",
    "    - Also the langchain team noticed that some components are used togther every times like prompts and llm, so instead of using prompts and llm seperately langchain provided a component called LLMChain with which it made easy for using llm and prompt with one single function.\n",
    "    - Likewise Langchain started providing such chain components like RetriverQA, SequentialChain, RouterChain, AgentExecutorChain, ConversationalRetrievalChain etc..\n",
    "\n",
    "![alt text](./Images/chainComponents.png)\n",
    "\n",
    "\n",
    "    - They started creating chain for tasks they are getting reused, so that instead of using separate components, user can use chains.\n",
    "    - The problem arised when, chains started getting popularity and there were so much chains that made langchain code base large and difficult to maintain.\n",
    "    - Since the components like models prompts are not standardised, the langchain team was forced to create such huge no.of chains.\n",
    "    - By standardised means, for using prompt component the user have to call prompt.format(), for using model we have to call llm.predict(), likewise each component had their own methods for using it.\n",
    "    - That's why they started creating custom functions and chains so that the user have not worry about this problem. But this was not a good practise.\n",
    "    - Since these components was not build to connect together and use, langchain has to write manual code for making it compatible.\n",
    "    - So the langchain started to standardise the components with the concept of **runnables**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b9beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NakliLLM:\n",
    "\n",
    "  def __init__(self):\n",
    "    print('LLM created')\n",
    "\n",
    "  def predict(self, prompt):\n",
    "\n",
    "    response_list = [\n",
    "        'Delhi is the capital of India',\n",
    "        'IPL is a cricket league',\n",
    "        'AI stands for Artificial Intelligence'\n",
    "    ]\n",
    "\n",
    "    return {'response': random.choice(response_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3dc6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NakliPromptTemplate:\n",
    "\n",
    "  def __init__(self, template, input_variables):\n",
    "    self.template = template\n",
    "    self.input_variables = input_variables\n",
    "\n",
    "  def format(self, input_dict):\n",
    "    return self.template.format(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493b7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = NakliPromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43dd5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format({'length':'short','topic':'india'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ca4d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "llm = NakliLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06bcc6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'IPL is a cricket league'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35665d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NakliLLMChain:\n",
    "\n",
    "  def __init__(self, llm, prompt):\n",
    "    self.llm = llm\n",
    "    self.prompt = prompt\n",
    "\n",
    "  def run(self, input_dict):\n",
    "\n",
    "    final_prompt = self.prompt.format(input_dict)\n",
    "    result = self.llm.predict(final_prompt)\n",
    "\n",
    "    return result['response']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e744c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = NakliPromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7cfd726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "llm = NakliLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73ec010",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = NakliLLMChain(llm, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb39edd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI stands for Artificial Intelligence'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run({'length':'short', 'topic': 'india'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e3772",
   "metadata": {},
   "source": [
    "# What"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb70189",
   "metadata": {},
   "source": [
    "![alt text](./Images/runnables.png)\n",
    "\n",
    "- We can think of runnables as a unit of work\n",
    "    - Each runnables will take a input, process it, and can provide a output\n",
    "- Every runnable in langchain have a common interface:\n",
    "    - invoke(), batch(), stream() method for calling it\n",
    "    - Also we can connect each runnables and it will form another, think of it like a lego block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ada88",
   "metadata": {},
   "source": [
    "# How"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252bd0e",
   "metadata": {},
   "source": [
    "- For creating common interface they used concept **abstract class and methods**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc630a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Runnables(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def invoke(input_data):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f97937dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class DummyLLM without an implementation for abstract method 'invoke'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m         response_list = [\n\u001b[32m      7\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mDelhi is the capital of India\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mIPL is a cricket league\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      9\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mAI stands for Artificial Intelligence\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m         ]\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m: random.choice(response_list)}\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m llm = \u001b[43mDummyLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Can't instantiate abstract class DummyLLM without an implementation for abstract method 'invoke'"
     ]
    }
   ],
   "source": [
    "class DummyLLM(Runnables):\n",
    "    def __init__(self):\n",
    "        print(\"LLM Created\")\n",
    "        \n",
    "    def predict(self, prompt):\n",
    "        response_list = [\n",
    "            'Delhi is the capital of India',\n",
    "            'IPL is a cricket league',\n",
    "            'AI stands for Artificial Intelligence'\n",
    "        ]\n",
    "\n",
    "        return {'response': random.choice(response_list)}\n",
    "\n",
    "llm = DummyLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "782ab80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Created\n"
     ]
    }
   ],
   "source": [
    "class DummyLLM(Runnables):\n",
    "    def __init__(self):\n",
    "        print(\"LLM Created\")\n",
    "    \n",
    "    # Abstract Method\n",
    "    def invoke(self, prompt):\n",
    "        \n",
    "        response_list = [\n",
    "            'Delhi is the capital of India',\n",
    "            'IPL is a cricket league',\n",
    "            'AI stands for Artificial Intelligence'\n",
    "        ]\n",
    "\n",
    "        return {'response': random.choice(response_list)}\n",
    "    \n",
    "    def predict(self, prompt):\n",
    "        response_list = [\n",
    "            'Delhi is the capital of India',\n",
    "            'IPL is a cricket league',\n",
    "            'AI stands for Artificial Intelligence'\n",
    "        ]\n",
    "\n",
    "        return {'response': random.choice(response_list)}\n",
    "    \n",
    "llm = DummyLLM()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb9b7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NakliPromptTemplate(Runnables):\n",
    "    def __init__(self, template, input_variables):\n",
    "        self.template = template\n",
    "        self.input_variables = input_variables\n",
    "\n",
    "    def invoke(self,input_dict):\n",
    "        return self.template.format(**input_dict)\n",
    "    \n",
    "    def format(self, input_dict):\n",
    "        return self.template.format(**input_dict)\n",
    "    \n",
    "template = NakliPromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76a95a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyStrOutputParser(Runnables):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def invoke(self,input_data):\n",
    "        return input_data['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "240f1ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnableConnector(Runnables):\n",
    "    def __init__(self,runnable_list):\n",
    "        self.runnable_list = runnable_list\n",
    "    \n",
    "    def invoke(self, input_data):\n",
    "        for runnable in self.runnable_list:\n",
    "            input_data = runnable.invoke(input_data)\n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "718c4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = DummyStrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44225c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableConnector([template,llm,parser])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6faa73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI stands for Artificial Intelligence'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'length':'short', 'topic': 'india'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b38bb3",
   "metadata": {},
   "source": [
    "### Connecting Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "115cd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = NakliPromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64f0e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = NakliPromptTemplate(\n",
    "    template='Explain the following joke {response}',\n",
    "    input_variables=['response']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "127cca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = RunnableConnector([template1, llm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "952edb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = RunnableConnector([template2, llm, parser])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a38d9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = RunnableConnector([chain1, chain2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e899956f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Delhi is the capital of India'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke({'topic':'cricket'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d241bf",
   "metadata": {},
   "source": [
    "# Langchain Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd08b771",
   "metadata": {},
   "source": [
    "![alt text](./Images/runnables_types.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "705e5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"qwen2.5-coder:7b\",\n",
    "    model_provider=\"ollama\",\n",
    "    temperature = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b11b3",
   "metadata": {},
   "source": [
    "# Runnable Passthrough\n",
    "\n",
    "- RunnablePassthroughis a special Runnable primitive that simply returns the input as output without modifying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "842a8c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': 'Why did the cricket player break up with his girlfriend?\\n\\nBecause he found out she was a bit of a no-show!', 'explanation': 'The joke is a play on words and cultural references. Cricket is a popular sport in many countries, particularly in England where this joke originates. The term \"no-show\" has two meanings: one is when someone doesn\\'t show up for an event or appointment, and the other is a slang term used to describe a woman who is sexually uninterested or unavailable.\\n\\nIn the context of the joke, the cricket player\\'s girlfriend breaking up with him because she was a \"bit of a no-show\" could be interpreted as her being unreliable or not showing up when expected. However, it could also be interpreted as her being sexually uninterested or unavailable, which is a play on the double meaning of \"no-show.\"\\n\\nThe punchline is unexpected and clever because it plays on the double meaning of \"no-show,\" making the joke both funny and culturally relevant.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Explain the following joke - {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt1, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'explanation': RunnableSequence(prompt2, model, parser)\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'cricket'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31199a49",
   "metadata": {},
   "source": [
    "# Runnable Sequence\n",
    "\n",
    "- RunnableSequence is a sequential chain of runnables in LangChain that executes each step one after another, passing the output of one step as the input to the next. \n",
    "- It is useful when you need to compose multiple runnables together in a structured workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13dfc7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joke is a play on words. \"AI\" stands for Artificial Intelligence, but it can also be read as \"A I,\" which sounds like \"I am.\" The punchline \"because it had a splitting headache!\" is a reference to the common phrase \"splitting headache,\" which means a very severe or intense headache. So, the joke is saying that the AI (which is actually just a computer program) went to therapy because it was experiencing a very severe headache.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Explain the following joke - {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "chain = RunnableSequence(prompt1, model, parser, prompt2, model, parser)\n",
    "\n",
    "print(chain.invoke({'topic':'AI'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb9901e",
   "metadata": {},
   "source": [
    "# Runnable Lambda\n",
    "\n",
    "- RunnableLambdais a runnable primitive that allows you to apply custom Python functions within an AI pipeline. \n",
    "- It acts as a middlewarebetween different AI components, enabling preprocessing, transformation, API calls, filtering, and post-processingin a LangChain workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8a4daeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI go to therapy?\n",
      "\n",
      "Because it had a splitting headache! \n",
      " word count - 13\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'word_count': RunnableLambda(word_count)\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "result = final_chain.invoke({'topic':'AI'})\n",
    "\n",
    "final_result = \"\"\"{} \\n word count - {}\"\"\".format(result['joke'], result['word_count'])\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf02362",
   "metadata": {},
   "source": [
    "# Runnable Branch\n",
    "\n",
    "- RunnableBranchis a control flow componentin LangChain that allows you to conditionally route input data to different chains or runnablesbased on custom logic. \n",
    "- It functions like an if/elif/elseblock for chains ‚Äîwhere you define a set of condition functions, each associated with a runnable (e.g., LLM call, prompt chain, or tool). The first matching condition is executed. If no condition matches, a default runnableis used (if provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a952c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary of the Report: Russia-Ukraine Conflict\n",
      "\n",
      "The Russia-Ukraine conflict is one of the most significant geopolitical events in recent years. The roots of this conflict can be traced back to historical, cultural, economic, and political factors. This report aims to provide a comprehensive overview of the conflict, including its history, key players, major events, and potential outcomes.\n",
      "\n",
      "#### Historical Context\n",
      "The relationship between Russia and Ukraine has been complex for centuries. Historically, both countries have shared a rich cultural heritage and linguistic ties. However, the modern state of Ukraine emerged in the late 19th century as part of the Russian Empire. Following the collapse of the Soviet Union in 1991, Ukraine gained independence but faced numerous challenges.\n",
      "\n",
      "#### Key Players\n",
      "- **Russia**: The primary actor is the Russian Federation, led by President Vladimir Putin.\n",
      "- **Ukraine**: An independent nation with its own government, military, and political system.\n",
      "\n",
      "#### Major Events\n",
      "1. **Crimea Annexation (2014)**: Russia annexed Crimea in 2014, citing security concerns.\n",
      "2. **Eastern Ukraine Conflict (2014-2023)**: Pro-Russian separatists have been fighting Ukrainian forces since 2014.\n",
      "\n",
      "#### Causes\n",
      "The conflict is multifaceted, with historical ties, economic interdependence, geopolitical competition, and nationalism as key causes.\n",
      "\n",
      "#### International Response\n",
      "The international community has responded with sanctions and peace talks. Various peace agreements have been held but have not led to a lasting resolution.\n",
      "\n",
      "#### Potential Outcomes\n",
      "Uncertain, depending on factors such as continued conflict, peace agreement, or external intervention.\n",
      "\n",
      "#### Conclusion\n",
      "The Russia-Ukraine conflict is complex and its outcome remains uncertain. The international community's response has included sanctions and peace talks, but a lasting resolution is elusive.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Summarize the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "report_gen_chain = prompt1 | model | parser\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: len(x.split())>300, prompt2 | model | parser),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "final_chain = RunnableSequence(report_gen_chain, branch_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'Russia vs Ukraine'}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2247f",
   "metadata": {},
   "source": [
    "# Runnable Parallel\n",
    "- RunnableParallel is a runnable primitive that allows multiple runnables  to execute in parallel. \n",
    "- Each runnable receives the same input and processes it independently, producing a dictionary of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51922d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Exciting times ahead as artificial intelligence continues to revolutionize our world! From healthcare to transportation, AI is making incredible strides in improving efficiency and enhancing human capabilities. #AI #Innovation #FutureIsNow\"\n",
      "üöÄ Exciting Times Ahead! üåü\n",
      "\n",
      "As we step into 2024, the world of Artificial Intelligence (AI) is on the cusp of revolutionizing industries and enhancing lives in ways we can only begin to imagine. From healthcare diagnostics to sustainable energy solutions, AI's impact is becoming increasingly evident.\n",
      "\n",
      "ü§ñ How AI is Shaping Our Future:\n",
      "- **Healthcare**: AI is helping doctors diagnose diseases more accurately and quickly, improving patient outcomes.\n",
      "- **Finance**: It's revolutionizing how banks operate, from fraud detection to personalized financial advice.\n",
      "- **Transportation**: Self-driving cars are not just a dream; they're closer than ever, promising safer roads and reduced traffic congestion.\n",
      "- **Education**: AI is personalizing learning experiences, making education more accessible and effective.\n",
      "\n",
      "üí° The Future of AI:\n",
      "AI's potential is vast, but it also comes with challenges. Ensuring ethical use, protecting privacy, and addressing the digital divide are crucial as we integrate AI into our daily lives.\n",
      "\n",
      "ü§ù Collaboration is Key:\n",
      "As AI continues to evolve, collaboration between technologists, policymakers, and industry leaders will be essential to harness its full benefits while mitigating risks.\n",
      "\n",
      "Let's embrace this exciting journey together! üåç‚ú®\n",
      "\n",
      "#AI #FutureTech #Innovation #EthicalAI\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Generate a tweet about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Generate a Linkedin post about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'tweet': RunnableSequence(prompt1, model, parser),\n",
    "    'linkedin': RunnableSequence(prompt2, model, parser)\n",
    "})\n",
    "\n",
    "result = parallel_chain.invoke({'topic':'AI'})\n",
    "\n",
    "print(result['tweet'])\n",
    "print(result['linkedin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce625325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
