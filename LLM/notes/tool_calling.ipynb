{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42e25cd",
   "metadata": {},
   "source": [
    "# Tool Binding\n",
    "\n",
    "- Tool binding is the process where you register tools with a LLM so that :\n",
    "    - The LLM knows what tools are available.\n",
    "    - It knows what each tool does (via description).\n",
    "    - It knowa what input format to use (via schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7361a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import HumanMessage, ToolMessage, AIMessage\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"functiongemma:latest\",\n",
    "    model_provider=\"ollama\",\n",
    "    temperature = 0.0\n",
    ")\n",
    "\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e2ac933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Creation\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "  \"\"\"Given 2 numbers a and b this tool returns their product\"\"\"\n",
    "  return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bf28849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am doing well, thank you for asking! I hope you are too. How can I assist you today?', additional_kwargs={}, response_metadata={'model': 'functiongemma:latest', 'created_at': '2026-02-21T10:00:56.7419709Z', 'done': True, 'done_reason': 'stop', 'total_duration': 518204300, 'load_duration': 245617700, 'prompt_eval_count': 98, 'prompt_eval_duration': 16447800, 'eval_count': 24, 'eval_duration': 240865900, 'logprobs': None, 'model_name': 'functiongemma:latest', 'model_provider': 'ollama'}, id='lc_run--019c7fa5-289e-7d82-b678-a6cc8de7e2d4-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 98, 'output_tokens': 24, 'total_tokens': 122})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tool Binding\n",
    "llm_withTools = model.bind_tools([multiply])\n",
    "llm_withTools.invoke('hi how are you')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae02260",
   "metadata": {},
   "source": [
    "# Tool Calling\n",
    "\n",
    "- Tool calling is the process where the LLM decides, during a conversation or task, that it needs to use a specific tool(function) - and generates a structured output with :\n",
    "    - name of the tool\n",
    "    - the arguments to call with it\n",
    "\n",
    "- The LLM does not actually run the tool- It just suggests the tool and argument. The actual execution is handled by Langchain for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16ba00b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('content', 'The product of 4 and 3 is 6.')\n",
      "('additional_kwargs', {})\n",
      "('response_metadata', {'model': 'functiongemma:latest', 'created_at': '2026-02-21T10:20:42.070878Z', 'done': True, 'done_reason': 'stop', 'total_duration': 634840600, 'load_duration': 322857000, 'prompt_eval_count': 152, 'prompt_eval_duration': 20450100, 'eval_count': 27, 'eval_duration': 260161500, 'logprobs': None, 'model_name': 'functiongemma:latest', 'model_provider': 'ollama'})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'lc_run--019c7fb7-3e5a-72b1-98d0-713ddb6b13fb-0')\n",
      "('tool_calls', [{'name': 'multiply', 'args': {'a': 4, 'b': 3}, 'id': '0dbd7d63-3f24-49f9-8c11-42e1d7ffad21', 'type': 'tool_call'}])\n",
      "('invalid_tool_calls', [])\n",
      "('usage_metadata', {'input_tokens': 152, 'output_tokens': 27, 'total_tokens': 179})\n"
     ]
    }
   ],
   "source": [
    "query = HumanMessage('can you multiply 4 and 3')\n",
    "messages.append(query)\n",
    "\n",
    "response = llm_withTools.invoke(messages)\n",
    "messages.append(response)\n",
    "for key in response:\n",
    "  print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43a819",
   "metadata": {},
   "source": [
    "# Tool Execution\n",
    "\n",
    "- Tool execution is the step where the actual python function(tool) is run using i/p args that LLM suggested during tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65ac9e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: multiply - ({'a': 4, 'b': 3})\n",
      "Result: 12\n"
     ]
    }
   ],
   "source": [
    "if response.tool_calls:\n",
    "  tool_msg = response.tool_calls[0]\n",
    "  print(f'Calling: {tool_msg['name']} - ({tool_msg['args']})')\n",
    "\n",
    "  result = multiply.invoke(tool_msg['args'])\n",
    "  print(f'Result: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdea8a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: multiply - ({'a': 4, 'b': 3})\n",
      "content='12' name='multiply' tool_call_id='0dbd7d63-3f24-49f9-8c11-42e1d7ffad21'\n",
      "<class 'langchain_core.messages.tool.ToolMessage'>\n"
     ]
    }
   ],
   "source": [
    "if response.tool_calls:\n",
    "  tool_msg = response.tool_calls[0]\n",
    "  print(f'Calling: {tool_msg['name']} - ({tool_msg['args']})')\n",
    "  result = multiply.invoke(tool_msg)\n",
    "  messages.append(result)\n",
    "  print(result)\n",
    "  print(type(result))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "604a5b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The product of 4 and 3 is 12.', additional_kwargs={}, response_metadata={'model': 'functiongemma:latest', 'created_at': '2026-02-21T10:20:42.9167047Z', 'done': True, 'done_reason': 'stop', 'total_duration': 488220300, 'load_duration': 328132300, 'prompt_eval_count': 189, 'prompt_eval_duration': 18085700, 'eval_count': 14, 'eval_duration': 131505100, 'logprobs': None, 'model_name': 'functiongemma:latest', 'model_provider': 'ollama'}, id='lc_run--019c7fb7-423b-7003-afdf-10ff8166e611-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 189, 'output_tokens': 14, 'total_tokens': 203})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result = llm_withTools.invoke(messages)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d722b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f0948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
