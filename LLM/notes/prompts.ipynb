{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886773a4",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "\n",
    "    - prompts are input instructions or queries to a model to guide its output\n",
    "- https://reference.langchain.com/python/langchain_core/prompts/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a6fd6",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f244387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df82fee8",
   "metadata": {},
   "source": [
    "## Advantages of prompt templates over f-string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c35bf1",
   "metadata": {},
   "source": [
    "### 1- Default Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f531a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='you are a helpfull assistant, who answer in 20 words, \\n explain about earth')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate(\n",
    "    template=\"you are a helpfull assistant, who answer in {output_limit} words, \\n {user_input}\",\n",
    "    input_variables=['output_limit','user_input']\n",
    ")\n",
    "\n",
    "query = 'explain about earth'\n",
    "\n",
    "template.invoke({\n",
    "    'output_limit' : 20,\n",
    "    'user_input' : query\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a062aec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'user_input'}.  Expected: ['output_limit', 'user_input'] Received: ['output_limit']\\nNote: if you intended {user_input} to be part of the string and not a variable, please escape it with double curly braces like: '{{user_input}}'.\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtemplate\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moutput_limit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Profession\\Coding\\Study\\LLM\\llm_env\\Lib\\site-packages\\langchain_core\\prompts\\base.py:223\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    222\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Profession\\Coding\\Study\\LLM\\llm_env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2060\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2056\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2057\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2058\u001b[39m         output = cast(\n\u001b[32m   2059\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2060\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2062\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2068\u001b[39m         )\n\u001b[32m   2069\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2070\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Profession\\Coding\\Study\\LLM\\llm_env\\Lib\\site-packages\\langchain_core\\runnables\\config.py:452\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    451\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Profession\\Coding\\Study\\LLM\\llm_env\\Lib\\site-packages\\langchain_core\\prompts\\base.py:196\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     inner_input_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**inner_input_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Profession\\Coding\\Study\\LLM\\llm_env\\Lib\\site-packages\\langchain_core\\prompts\\base.py:190\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    184\u001b[39m     example_key = missing.pop()\n\u001b[32m    185\u001b[39m     msg += (\n\u001b[32m    186\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    188\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    191\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    192\u001b[39m     )\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input_\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to PromptTemplate is missing variables {'user_input'}.  Expected: ['output_limit', 'user_input'] Received: ['output_limit']\\nNote: if you intended {user_input} to be part of the string and not a variable, please escape it with double curly braces like: '{{user_input}}'.\\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/INVALID_PROMPT_INPUT \""
     ]
    }
   ],
   "source": [
    "template.invoke({\n",
    "    'output_limit' : 20,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb9e6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='you are a helpfull assistant, who answer in 20 words, \\n explain about earth')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_format = 'joke'\n",
    "template.invoke({\n",
    "    'output_limit' : 20,\n",
    "    'user_input' : query,\n",
    "    'type' : answer_format\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b51613",
   "metadata": {},
   "source": [
    "### 2 - Reusable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d46c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "template.save('saved_prompt.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b47a0048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['output_limit', 'user_input'], input_types={}, partial_variables={}, template='you are a helpfull assistant, who answer in {output_limit} words, \\n {user_input}')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt('./saved_prompt.json')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bbb30",
   "metadata": {},
   "source": [
    "### 3 - Coupled With Langchain Ecosystem\n",
    "\n",
    "    - Which helps in creating complex chain/worksflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83524f4c",
   "metadata": {},
   "source": [
    "# Text prompts\n",
    "\n",
    "    - Text prompts are strings - ideal for straightforward generation tasks where you don’t need to retain conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660b8bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Spring's soft whispers bloom,\\nNature dances with the light,\\nNew life springs from the earth.\" additional_kwargs={} response_metadata={'model': 'qwen2.5-coder:7b', 'created_at': '2026-02-19T17:41:24.3913274Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4119650600, 'load_duration': 94712700, 'prompt_eval_count': 35, 'prompt_eval_duration': 1705690900, 'eval_count': 20, 'eval_duration': 2258071700, 'logprobs': None, 'model_name': 'qwen2.5-coder:7b', 'model_provider': 'ollama'} id='lc_run--019c76fd-f30c-7380-aa22-acf1c7774485-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 35, 'output_tokens': 20, 'total_tokens': 55}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"qwen2.5-coder:7b\",\n",
    "    model_provider=\"ollama\",\n",
    "    temperature = 0.0\n",
    ")\n",
    "\n",
    "response = model.invoke(\"Write a haiku about spring\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060692e5",
   "metadata": {},
   "source": [
    "### Use text prompts when:\n",
    "    - You have a single, standalone request\n",
    "    - You don’t need conversation history\n",
    "    - You want minimal code complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd5b38",
   "metadata": {},
   "source": [
    "# Message prompts\n",
    "\n",
    "    - Alternatively, you can pass in a list of messages to the model by providing a list of message objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521e4b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"  \\nSoft whispers of warm sunlight—  \\nSpring's gentle embrace.\" additional_kwargs={} response_metadata={'model': 'qwen2.5-coder:7b', 'created_at': '2026-02-19T17:42:57.8042382Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3145352600, 'load_duration': 107247500, 'prompt_eval_count': 30, 'prompt_eval_duration': 1446813800, 'eval_count': 14, 'eval_duration': 1536459400, 'logprobs': None, 'model_name': 'qwen2.5-coder:7b', 'model_provider': 'ollama'} id='lc_run--019c76ff-63c1-75b3-922d-8c29d97861d0-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 30, 'output_tokens': 14, 'total_tokens': 44}\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a poetry expert\"),\n",
    "    HumanMessage(\"Write a haiku about spring\"),\n",
    "    AIMessage(\"Cherry blossoms bloom...\")\n",
    "]\n",
    "response = model.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7754a91",
   "metadata": {},
   "source": [
    "### Use message prompts when:\n",
    "    - Managing multi-turn conversations\n",
    "    - Working with multimodal content (images, audio, files)\n",
    "    - Including system instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe24a71",
   "metadata": {},
   "source": [
    "## Dictionary Format\n",
    "\n",
    "    - You can also specify messages directly in OpenAI chat completions format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ced7e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"  \\nSoft whispers of warm sunlight—  \\nSpring's gentle embrace.\" additional_kwargs={} response_metadata={'model': 'qwen2.5-coder:7b', 'created_at': '2026-02-19T17:50:55.1986349Z', 'done': True, 'done_reason': 'stop', 'total_duration': 11398720300, 'load_duration': 8001956300, 'prompt_eval_count': 30, 'prompt_eval_duration': 1765058600, 'eval_count': 14, 'eval_duration': 1597069000, 'logprobs': None, 'model_name': 'qwen2.5-coder:7b', 'model_provider': 'ollama'} id='lc_run--019c7706-8c56-7573-99ab-5acd9a119e11-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 30, 'output_tokens': 14, 'total_tokens': 44}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetry expert\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a haiku about spring\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Cherry blossoms bloom...\"}\n",
    "]\n",
    "response = model.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ebbee",
   "metadata": {},
   "source": [
    "# ChatPromptTemplate\n",
    "\n",
    "    - Prompt template for chat models.\n",
    "    - Use to create flexible templated prompts for chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00216e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='My name is Bob.' additional_kwargs={} response_metadata={'model': 'qwen2.5-coder:7b', 'created_at': '2026-02-19T17:54:10.4598271Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1493493100, 'load_duration': 101684600, 'prompt_eval_count': 54, 'prompt_eval_duration': 741066700, 'eval_count': 6, 'eval_duration': 637793800, 'logprobs': None, 'model_name': 'qwen2.5-coder:7b', 'model_provider': 'ollama'} id='lc_run--019c7709-adc5-7d31-8c4e-c121dcd3ea43-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 54, 'output_tokens': 6, 'total_tokens': 60}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2f512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
